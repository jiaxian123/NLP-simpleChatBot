{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U7JT5qlx8t6"
      },
      "source": [
        "# install tflearn in google colab\n",
        "!pip install -q tflearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIofoq-gyCKw"
      },
      "source": [
        "# install nltk in google colab\n",
        "!pip install -q nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgCW40D3q08V"
      },
      "source": [
        "!pip install -q tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyJUwUtCfUPX"
      },
      "source": [
        "#Used in Tensorflow Model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tflearn\n",
        "import random\n",
        "\n",
        "#Used to for Contextualisation and Other NLP Tasks.\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "\n",
        "import json #for import file\n",
        "import pickle #for serializing the structure\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #to ignore the warning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH1G3WAGfUPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ee1514-36ed-4ad6-f890-bee5e93cd828"
      },
      "source": [
        "print(\"Processing the Intents.....\")\n",
        "# load the json file in google colab storage\n",
        "with open('/content/assignmentNLP.json') as json_data: \n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing the Intents.....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMq8JrIogVtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c19b997-8e31-4209-87eb-40bdd9ca7e76"
      },
      "source": [
        "#for later tokenize sentence\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EceJgHxifUPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6702d414-f178-4913-a209-3e67e86c1df1"
      },
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?','!','.']\n",
        "print(\"Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\")\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in our corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlpYz4XvfUPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933b49fd-e881-413b-ae22-f1f91362102e"
      },
      "source": [
        "print(\"Stemming, Lowering and Removing Duplicates.......\")\n",
        "#change the word to lower case and stemming the words\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming, Lowering and Removing Duplicates.......\n",
            "216 documents\n",
            "101 classes ['Age', 'Alopecia areata', 'BestWay', 'Biotin', 'COVIDTE', 'CSbaldness', 'Cancer', 'Cause', 'Covid', 'Depression', 'Diabetes', 'Dieting', 'ExcercideTE', 'ExcessVit', 'FTE', 'Fever', 'Gphase', 'Growingback', 'HCT', 'HTUC', 'HWAMHCPD', 'HairColoring', 'HairCutQ', 'HairFallingDown1', 'HairLife', 'Iron', 'LowBiotin', 'Lupus', 'MHairGrow', 'Mbalness', 'PBaldnessS', 'PSummerHL', 'Psoriasis', 'Rphase', 'STD', 'STE', 'Shampoo', 'StopHL', 'Stress', 'SummerHL', 'Supplement', 'TCSbaldness', 'TFFB', 'TFMB', 'Telogeneffluvium', 'Thyroid Issues', 'Vitamin A', 'VitaminC', 'VitaminD', 'WDTSGGO', 'WSD', 'WTE', 'WashHairQ', 'Wbalness', 'Zinc', 'avoid', 'baldness', 'deal', 'disease', 'doctor', 'doctorType', 'emotional trauma', 'factors', 'food', 'goodbye', 'greeting', 'growBack', 'hair styling', 'hairLossVit', 'hairline', 'healthCondition', 'hours', 'infections', 'issueTE', 'itchy', 'lessIron', 'medication', 'medicationsCause', 'menAge', 'myths', 'name', 'normalHairLoss', 'notPreventable', 'phases', 'pregnancy', 'preventTE', 'products', 'reducefrizzQ', 'riskFactors', 'signs', 'surgery', 'sypmtoms', 'takeIron', 'takeVitC', 'takeVitD', 'takeZinc', 'thanks', 'tractionAlopecia', 'trauma', 'wihl', 'women']\n",
            "268 unique stemmed words [\"'s\", ',', '100', '50', 'a', 'about', 'affect', 'after', 'age', 'aggit', 'ahir', 'aid', 'alopecia', 'and', 'ani', 'anyon', 'are', 'area', 'associ', 'at', 'autoimmun', 'avoid', 'b7', 'babi', 'back', 'bal', 'bald', 'bankrupt', 'be', 'best', 'biotin', 'birth', 'blocker', 'blow-dri', 'bye', 'c', 'calcium', 'call', 'can', 'cancer', 'care', 'caus', 'cereal', 'channel', 'chemic', 'chemotherapi', 'chlorin', 'circumst', 'cold', 'color', 'colour', 'combat', 'condit', 'condition', 'constant', 'consult', 'covid', 'cut', 'd', 'dairi', 'damag', 'day', 'deal', 'defici', 'depress', 'diabet', 'diet', 'differ', 'diseas', 'divorc', 'do', 'doctor', 'doe', 'done', 'down', 'dp', 'drop', 'drug', 'eat', 'effect', 'effluvium', 'excercis', 'excess', 'experi', 'experienc', 'factor', 'fall', 'faster', 'fe', 'feel', 'female-pattern', 'fever', 'fibros', 'finer', 'fish', 'food', 'for', 'fortifi', 'frizz', 'frontal', 'gave', 'genet', 'get', 'go', 'good', 'goodby', 'greasi', 'grow', 'growth', 'had', 'hair', 'hairdo', 'hairlin', 'hairloss', 'hairstyl', 'happen', 'has', 'have', 'health', 'heat', 'hello', 'help', 'hi', 'hiv', 'hour', 'how', 'hypothyroid', 'i', 'if', 'illnes', 'impact', 'in', 'increas', 'indic', 'infect', 'iron', 'is', 'issu', 'itchi', 'lack', 'later', 'lead', 'less', 'life', 'like', 'live', 'liver', 'long', 'lose', 'loss', 'low', 'lupus', 'm', 'make', 'male-pattern', 'man', 'mani', 'may', 'me', 'mean', 'medic', 'men', 'mental', 'more', 'most', 'much', 'my', 'myth', 'name', 'need', 'normal', 'not', 'nsaid', 'occur', 'of', 'often', 'old', 'on', 'open', 'oper', 'or', 'out', 'patient', 'peopl', 'percentag', 'phase', 'physic', 'plaqu', 'pregnant', 'prevent', 'procedur', 'product', 'psoriasi', 'pull', 'real', 'reced', 'reduc', 'rest', 'retinoid', 'rich', 'risk', 'sad', 'salt', 'scalp', 'see', 'see-through', 'shampoo', 'shape', 'shed', 'shorter', 'should', 'sign', 'start', 'std', 'sti', 'stop', 'stress', 'style', 'summer', 'supplement', 'surgeri', 'surgic', 'symptom', 'sympton', 'syphili', 'take', 'te', 'telogen', 'thank', 'that', 'the', 'there', 'thin', 'thing', 'thinner', 'this', 'thyroid', 'tight', 'tip', 'to', 'too', 'tool', 'top', 'trauma', 'treat', 'treatment', 'type', 'typic', 'under', 'use', 'vitamin', 'wash', 'water', 'way', 'we', 'what', 'when', 'where', 'whi', 'which', 'who', 'widen', 'will', 'with', 'women', 'you', 'your', 'zinc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poZuXgz7fUPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1575abd-65bf-40e7-e654-f11b552ec664"
      },
      "source": [
        "print(\"Creating the Data for our Model.....\")\n",
        "training = []\n",
        "output = []\n",
        "print(\"Creating an List (Empty) for Output.....\")\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "print(\"Creating Traning Set, Bag of Words for our Model....\")\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the Data for our Model.....\n",
            "Creating an List (Empty) for Output.....\n",
            "Creating Traning Set, Bag of Words for our Model....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neBPIfXKfUPc"
      },
      "source": [
        "#Shuffling Randomly and Converting into Numpy Array for Faster Processing\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "#Create train list \n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "\n",
        "#Clear the pattern graphics and reset the replacement graphics\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDbAANDMfUPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf9de2c-84dc-414b-990f-6e7b17c4ccd7"
      },
      "source": [
        "# Build neural network\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])]) \n",
        "net = tflearn.fully_connected(net, 16)#the number of neurons in each layer are: 16\n",
        "net = tflearn.fully_connected(net, 16)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax') \n",
        "net = tflearn.regression(net,optimizer='adam',\n",
        "                             learning_rate=0.01)\n",
        "print(\"Training....\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAiQFBO9fUPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dcfe3df-bee2-4150-f7a5-5b282a6b64ca"
      },
      "source": [
        "model = tflearn.DNN(net, tensorboard_verbose=3)\n",
        "print(\"DOne\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DOne\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAFBXPa-fUPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b54729c-01c0-4c06-ae13-0a5ad63fd9ce"
      },
      "source": [
        "print(\"Training the Model.......\")\n",
        "model.fit(train_x, train_y, n_epoch=500, batch_size=16, show_metric=True)\n",
        "print(\"Saving the Model.......\")\n",
        "model.save('model.tflearn')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 6999  | total loss: \u001b[1m\u001b[32m0.35882\u001b[0m\u001b[0m | time: 0.134s\n",
            "| Adam | epoch: 500 | loss: 0.35882 - acc: 0.9552 -- iter: 208/216\n",
            "Training Step: 7000  | total loss: \u001b[1m\u001b[32m0.39733\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 500 | loss: 0.39733 - acc: 0.9534 -- iter: 216/216\n",
            "--\n",
            "Saving the Model.......\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHmzcea7fUPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c418377-edec-438d-d264-00237c5c0a98"
      },
      "source": [
        "print(\"Pickle is also Saved..........\")\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pickle is also Saved..........\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEJZbyUSfUPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bae8db-7e21-4677-e844-54b46cde4d53"
      },
      "source": [
        "print(\"Loading Pickle.....\")\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']\n",
        "\n",
        "\n",
        "with open('/content/assignmentNLP.json') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "    \n",
        "print(\"Loading the Model......\")\n",
        "# load our saved model\n",
        "model.load('./model.tflearn')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Pickle.....\n",
            "Loading the Model......\n",
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csGocR7KfUPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b476bcd1-e2b3-45fb-e4ad-49c8e97b8d9b"
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # It Tokenize or Break it into the constituents parts of Sentense.\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # Stemming means to find the root of the word.\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# Return the Array of Bag of Words: True or False and 0 or 1 for each word of bag that exists in the Sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))\n",
        "\n",
        "ERROR_THRESHOLD = 0.50\n",
        "print(\"ERROR_THRESHOLD = 0.50\")\n",
        "\n",
        "def classify(sentence):\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    print(results)\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1])) #Tuppl -> Intent and Probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    if results:\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)\n",
        "\n",
        "    else:\n",
        "      print(\"Sorry I'm not understand your question please write it more details \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR_THRESHOLD = 0.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHJtzKz0fUPh"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LmqRicNfUPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d713d4-7a52-42ee-aed4-4874e5470102"
      },
      "source": [
        "while True:\n",
        "    input_data = input(\"You- \")\n",
        "    answer = response(input_data)\n",
        "    answer\n",
        "    if input_data == (\"goodbye\" or \"bye\" or \"Goodbye\"):\n",
        "      break;\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You- what is baldness\n",
            "[[56, 0.9967301]]\n",
            "Baldness usually refers to excessive hair loss on the scalp. Hereditary hair loss with age is the most common cause of baldness.\n",
            "You- sfdsufhsdfsdf\n",
            "[]\n",
            "Sorry I'm not understand your question please write it more details \n",
            "You- goodbye\n",
            "[[64, 0.99942845]]\n",
            "See you later, thanks for visiting\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}